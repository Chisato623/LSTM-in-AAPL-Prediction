{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":976925,"sourceType":"datasetVersion","datasetId":533900}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n# Optimized AAPL Stock Price Prediction Model\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn.functional as F\nimport os\nfrom datetime import datetime\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set Matplotlib to use default font\nplt.rcParams['font.family'] = 'DejaVu Sans'\n\n# Data loading and preprocessing function\ndef load_data(file_path, sequence_length=60, add_technical_indicators=True):\n    \"\"\"\n    Load and preprocess stock data\n    \n    Parameters:\n        file_path: Path to the data file\n        sequence_length: Input sequence length\n        add_technical_indicators: Whether to add technical indicators\n        \n    Returns:\n        Preprocessed data and related objects\n    \"\"\"\n    print(\"Loading AAPL historical stock data...\")\n    \n    # Read CSV file\n    df = pd.read_csv('HistoricalQuotes.csv')\n    \n    # Clean column names\n    df.columns = df.columns.str.strip()\n    \n    # Data preview\n    print(f\"\\nData shape: {df.shape}\")\n    print(\"\\nFirst 5 rows:\")\n    print(df.head())\n    \n    # Data cleaning\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Process price columns\n    price_columns = ['Close/Last', 'Open', 'High', 'Low']\n    for col in price_columns:\n        if col in df.columns:\n            if df[col].astype(str).str.contains('$').any():\n                df[col] = df[col].str.replace('$', '', regex=False).str.replace(',', '', regex=False).astype(float)\n            else:\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    # Process volume column\n    if 'Volume' in df.columns:\n        df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')\n    \n    # Rename columns\n    if 'Close/Last' in df.columns:\n        df = df.rename(columns={'Close/Last': 'Close'})\n    \n    # Sort by date\n    df = df.sort_values('Date').reset_index(drop=True)\n    \n    # Calculate technical indicators\n    if add_technical_indicators:\n        print(\"\\nCalculating technical indicators...\")\n        \n        # Moving averages\n        df['MA5'] = df['Close'].rolling(window=5).mean()\n        df['MA10'] = df['Close'].rolling(window=10).mean()\n        df['MA20'] = df['Close'].rolling(window=20).mean()\n        df['MA50'] = df['Close'].rolling(window=50).mean()\n        \n        # Relative Strength Index (RSI)\n        def calculate_rsi(data, window=14):\n            delta = data.diff()\n            gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n            loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n            rs = gain / loss.replace(0, 1e-10)\n            rsi = 100 - (100 / (1 + rs))\n            return rsi\n        \n        df['RSI'] = calculate_rsi(df['Close'], window=14)\n        \n        # MACD\n        df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean()\n        df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean()\n        df['MACD'] = df['EMA12'] - df['EMA26']\n        df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n        \n        # Bollinger Bands\n        df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n        df['BB_Std'] = df['Close'].rolling(window=20).std()\n        df['BB_Upper'] = df['BB_Middle'] + 2 * df['BB_Std']\n        df['BB_Lower'] = df['BB_Middle'] - 2 * df['BB_Std']\n        \n        # Volume change rate\n        df['Volume_Change'] = df['Volume'].pct_change()\n        \n        # Price change rate\n        df['Price_Change'] = df['Close'].pct_change()\n    \n    # Select required feature columns\n    basic_features = ['Close', 'Open', 'High', 'Low', 'Volume']\n    if add_technical_indicators:\n        technical_features = ['MA5', 'MA10', 'MA20', 'MA50', 'RSI', 'MACD', 'Signal', \n                            'BB_Middle', 'BB_Upper', 'BB_Lower', 'Volume_Change', 'Price_Change']\n        feature_columns = basic_features + technical_features\n    else:\n        feature_columns = basic_features\n    \n    data = df[['Date'] + feature_columns].copy()\n    \n    print(f\"\\nCleaned data shape: {data.shape}\")\n    print(\"\\nBasic data info:\")\n    print(data.info())\n    \n    # Check missing values\n    print(\"\\nMissing values check:\")\n    print(data.isnull().sum())\n    \n    # Handle missing values\n    data = data.dropna().reset_index(drop=True)\n    \n    # Data normalization\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    scaled_features = scaler.fit_transform(data[feature_columns])\n    \n    # Create normalized dataframe\n    scaled_df = pd.DataFrame(scaled_features, columns=feature_columns)\n    scaled_df['Date'] = data['Date'].values\n    \n    print(\"\\nData normalization completed!\")\n    print(f\"\\nFeature columns used ({len(feature_columns)}):\")\n    print(feature_columns)\n    \n    return data, scaled_df, scaler, feature_columns\n\n# Create sequence dataset\ndef create_sequences(data, sequence_length, feature_columns):\n    \"\"\"\n    Create time series dataset\n    \n    Parameters:\n        data: Normalized data\n        sequence_length: Sequence length\n        feature_columns: Feature columns to use\n        \n    Returns:\n        Training and testing datasets\n    \"\"\"\n    scaled_data = data[feature_columns].values\n    \n    # Create train/test split (80% train, 20% test)\n    train_size = int(len(scaled_data) * 0.8)\n    train_data = scaled_data[:train_size]\n    test_data = scaled_data[train_size:]\n    \n    print(f\"Total data points: {len(scaled_data)}\")\n    print(f\"Training data size: {len(train_data)} ({train_size/len(scaled_data)*100:.1f}%)\")\n    print(f\"Testing data size: {len(test_data)} ({(len(scaled_data)-train_size)/len(scaled_data)*100:.1f}%)\")\n    \n    # Create sequences\n    def _create_sequences(data, seq_len):\n        X, y = [], []\n        for i in range(len(data) - seq_len):\n            X.append(data[i:i+seq_len])\n            y.append(data[i+seq_len, 0])  # Predict close price (first feature)\n        return np.array(X), np.array(y)\n    \n    X_train, y_train = _create_sequences(train_data, sequence_length)\n    X_test, y_test = _create_sequences(test_data, sequence_length)\n    \n    print(f\"\\nTraining set shapes - X: {X_train.shape}, y: {y_train.shape}\")\n    print(f\"Testing set shapes - X: {X_test.shape}, y: {y_test.shape}\")\n    \n    # Convert to PyTorch tensors\n    X_train_tensor = torch.FloatTensor(X_train)\n    y_train_tensor = torch.FloatTensor(y_train).squeeze()\n    X_test_tensor = torch.FloatTensor(X_test)\n    y_test_tensor = torch.FloatTensor(y_test).squeeze()\n    \n    print(f\"\\nTensor shape verification:\")\n    print(f\"X_train_tensor: {X_train_tensor.shape}\")\n    print(f\"y_train_tensor: {y_train_tensor.shape}\")\n    print(f\"X_test_tensor: {X_test_tensor.shape}\")\n    print(f\"y_test_tensor: {y_test_tensor.shape}\")\n    \n    # Create data loaders\n    batch_size = 32\n    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    print(\"\\nData preparation completed!\")\n    \n    return (\n        train_loader, test_loader, \n        X_train_tensor, y_train_tensor, \n        X_test_tensor, y_test_tensor,\n        train_size, sequence_length\n    )\n\n# Optimized GRU model (simplified version)\nclass OptimizedGRU(nn.Module):\n    \"\"\"Simplified GRU model with optimal hyperparameters\"\"\"\n    def __init__(self, input_size, hidden_size=32, num_layers=1, dropout=0.1):\n        super(OptimizedGRU, self).__init__()\n        \n        # Simplified GRU architecture\n        self.gru = nn.GRU(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Single fully connected layer\n        self.fc = nn.Linear(hidden_size, 1)\n    \n    def forward(self, x):\n        out, _ = self.gru(x)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out.squeeze()\n\n# Optimized LSTM model (simplified)\nclass OptimizedLSTM(nn.Module):\n    \"\"\"Simplified LSTM model with optimal hyperparameters\"\"\"\n    def __init__(self, input_size, hidden_size=32, num_layers=1, dropout=0.1):\n        super(OptimizedLSTM, self).__init__()\n        \n        # Simplified LSTM architecture\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Single fully connected layer\n        self.fc = nn.Linear(hidden_size, 1)\n    \n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out.squeeze()\n\n# Optimized Transformer model (simplified)\nclass OptimizedTransformer(nn.Module):\n    \"\"\"Simplified Transformer model with optimal hyperparameters\"\"\"\n    def __init__(self, input_size, hidden_size=32, num_layers=1, num_heads=2, dropout=0.1):\n        super(OptimizedTransformer, self).__init__()\n        \n        # Input projection\n        self.input_proj = nn.Linear(input_size, hidden_size)\n        \n        # Transformer encoder\n        encoder_layers = nn.TransformerEncoderLayer(\n            d_model=hidden_size, \n            nhead=num_heads, \n            dropout=dropout,\n            dim_feedforward=hidden_size*2\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n        \n        # Output layer\n        self.fc = nn.Linear(hidden_size, 1)\n    \n    def forward(self, x):\n        # Input projection\n        x = self.input_proj(x)\n        \n        # Transformer expects (seq_len, batch, features)\n        x = x.permute(1, 0, 2)\n        \n        # Transformer forward pass\n        out = self.transformer_encoder(x)\n        \n        # Take last token output\n        out = out.permute(1, 0, 2)[:, -1, :]\n        \n        # Output layer\n        out = self.fc(out)\n        return out.squeeze()\n\n# Optimized Attention LSTM model (simplified)\nclass OptimizedAttentionLSTM(nn.Module):\n    \"\"\"Simplified Attention LSTM model with optimal hyperparameters\"\"\"\n    def __init__(self, input_size, hidden_size=32, num_layers=1, dropout=0.1):\n        super(OptimizedAttentionLSTM, self).__init__()\n        \n        # LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Attention mechanism\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_size, 1),\n            nn.Softmax(dim=1)\n        )\n        \n        # Output layer\n        self.fc = nn.Linear(hidden_size, 1)\n    \n    def forward(self, x):\n        # LSTM forward pass\n        out, _ = self.lstm(x)\n        \n        # Attention weights\n        attention_weights = self.attention(out)\n        \n        # Weighted sum\n        weighted_out = torch.sum(out * attention_weights, dim=1)\n        \n        # Output layer\n        out = self.fc(weighted_out)\n        return out.squeeze()\n\n# Train model\ndef train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, model_name, epochs=50, patience=10):\n    \"\"\"\n    Train model\n    \n    Parameters:\n        model: Model to train\n        train_loader: Training data loader\n        test_loader: Testing data loader\n        criterion: Loss function\n        optimizer: Optimizer\n        scheduler: Learning rate scheduler\n        device: Device (CPU/GPU)\n        model_name: Model name (for saving)\n        epochs: Number of training epochs\n        patience: Early stopping patience\n        \n    Returns:\n        Trained model and loss history\n    \"\"\"\n    model = model.to(device)\n    \n    train_losses = []\n    test_losses = []\n    best_test_loss = float('inf')\n    patience_counter = 0\n    \n    print(f\"Starting training {model_name}...\")\n    \n    for epoch in range(epochs):\n        # Training mode\n        model.train()\n        train_loss = 0.0\n        \n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            \n            optimizer.zero_grad()\n            \n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            train_loss += loss.item() * batch_X.size(0)\n        \n        train_loss /= len(train_loader.dataset)\n        train_losses.append(train_loss)\n        \n        # Validation mode\n        model.eval()\n        test_loss = 0.0\n        \n        with torch.no_grad():\n            for batch_X, batch_y in test_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                \n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                \n                test_loss += loss.item() * batch_X.size(0)\n        \n        test_loss /= len(test_loader.dataset)\n        test_losses.append(test_loss)\n        \n        scheduler.step(test_loss)\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n        \n        if test_loss < best_test_loss:\n            best_test_loss = test_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), f'best_model_{model_name}.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping triggered. Best test loss: {best_test_loss:.6f}\")\n                break\n    \n    model.load_state_dict(torch.load(f'best_model_{model_name}.pth'))\n    \n    return model, train_losses, test_losses\n\n# Generate predictions\ndef make_predictions(model, data_loader, scaler, device, feature_columns):\n    \"\"\"Generate predictions and inverse scaling\"\"\"\n    model.eval()\n    predictions = []\n    actuals = []\n    \n    with torch.no_grad():\n        for batch_X, batch_y in data_loader:\n            batch_X = batch_X.to(device)\n            outputs = model(batch_X)\n            predictions.extend(outputs.cpu().numpy())\n            actuals.extend(batch_y.cpu().numpy())\n    \n    # Inverse scaling\n    predictions = np.array(predictions).reshape(-1, 1)\n    actuals = np.array(actuals).reshape(-1, 1)\n    \n    dummy_pred = np.zeros((len(predictions), len(feature_columns)))\n    dummy_pred[:, 0] = predictions.flatten()\n    predictions = scaler.inverse_transform(dummy_pred)[:, 0]\n    \n    dummy_actual = np.zeros((len(actuals), len(feature_columns)))\n    dummy_actual[:, 0] = actuals.flatten()\n    actuals = scaler.inverse_transform(dummy_actual)[:, 0]\n    \n    return predictions, actuals\n\n# Multi-model comparison experiment\ndef run_model_comparison(models_to_compare, train_loader, test_loader, X_train, y_train, \n                        X_test, y_test, scaler, device, feature_columns):\n    \"\"\"\n    Run multi-model comparison experiment\n    \n    Parameters:\n        models_to_compare: List of models to compare\n        train_loader: Training data loader\n        test_loader: Testing data loader\n        X_train: Training features\n        y_train: Training targets\n        X_test: Testing features\n        y_test: Testing targets\n        scaler: Data scaler\n        device: Device (CPU/GPU)\n        feature_columns: Feature column list\n        \n    Returns:\n        Model comparison results dictionary\n    \"\"\"\n    print(\"\\n=== Starting Multi-Model Comparison Experiment ===\")\n    \n    model_results = {}\n    \n    for model_name, model_class, model_params in models_to_compare:\n        print(f\"\\n--- Training Model: {model_name} ---\")\n        \n        model = model_class(**model_params)\n        print(\"Model Architecture:\")\n        print(model)\n        \n        criterion = nn.MSELoss()\n        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n        \n        model, train_losses, test_losses = train_model(\n            model, train_loader, test_loader, criterion, optimizer, scheduler,\n            device, model_name, epochs=50, patience=10\n        )\n        \n        predictions, actuals = make_predictions(model, test_loader, scaler, device, feature_columns)\n        \n        metrics = calculate_metrics(actuals, predictions)\n        \n        model_results[model_name] = {\n            'model': model,\n            'metrics': metrics,\n            'predictions': predictions,\n            'actuals': actuals,\n            'train_losses': train_losses,\n            'test_losses': test_losses\n        }\n        \n        print(f\"\\n{model_name} Evaluation Results:\")\n        for metric, value in metrics.items():\n            if isinstance(value, (int, float)):\n                print(f\"{metric}: {value:.4f}\")\n            else:\n                print(f\"{metric}: {value}\")\n    \n    print(\"\\n=== Multi-Model Comparison Completed ===\")\n    \n    return model_results\n\n# Calculate evaluation metrics\ndef calculate_metrics(actual, predicted):\n    \"\"\"Calculate evaluation metrics\"\"\"\n    mse = mean_squared_error(actual, predicted)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(actual, predicted)\n    medae = np.median(np.abs(actual - predicted))\n    r2 = r2_score(actual, predicted)\n    \n    # Calculate MAPE (avoid division by zero)\n    mask = actual != 0\n    mape = np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100\n    \n    # Calculate SMAPE\n    denominator = (np.abs(actual) + np.abs(predicted)) / 2.0\n    smape = np.mean(np.where(denominator != 0, np.abs(actual - predicted) / denominator, 0)) * 100\n    \n    # Calculate accuracy (prediction direction)\n    actual_direction = np.diff(actual) > 0\n    predicted_direction = np.diff(predicted) > 0\n    direction_accuracy = np.mean(actual_direction == predicted_direction) * 100\n    \n    return {\n        'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MedAE': medae, \n        'R2': r2, 'MAPE': mape, 'SMAPE': smape, 'Direction Accuracy': direction_accuracy\n    }\n\n# Main function\nif __name__ == \"__main__\":\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Generate timestamp for unique filenames\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Load and preprocess data\n    data, scaled_df, scaler, feature_columns = load_data(\n        'HistoricalQuotes.csv',\n        sequence_length=60,\n        add_technical_indicators=True\n    )\n    \n    # Create sequence dataset\n    (train_loader, test_loader, \n     X_train_tensor, y_train_tensor, \n     X_test_tensor, y_test_tensor,\n     train_size, sequence_length) = create_sequences(\n        scaled_df, \n        sequence_length=60,\n        feature_columns=feature_columns\n    )\n    \n    # Prepare raw data for analysis\n    X_train = X_train_tensor.cpu().numpy()\n    y_train = y_train_tensor.cpu().numpy()\n    X_test = X_test_tensor.cpu().numpy()\n    y_test = y_test_tensor.cpu().numpy()\n    \n    # Create time indices\n    test_start_idx = train_size + sequence_length\n    test_end_idx = test_start_idx + len(y_test)\n    test_dates = data['Date'].iloc[test_start_idx:test_end_idx].values\n    \n    # 1. Feature importance analysis\n    print(\"\\n=== Feature Importance Analysis ===\")\n    input_size = len(feature_columns)\n    \n    # Train a base model for feature analysis\n    base_model = OptimizedGRU(input_size, hidden_size=32, num_layers=1, dropout=0.1).to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(base_model.parameters(), lr=0.001, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n    \n    base_model, _, _ = train_model(base_model, train_loader, test_loader, criterion, optimizer, scheduler,\n                                  device, \"BaseModel\", epochs=30, patience=5)\n    \n    # Analyze feature importance\n    def analyze_feature_importance(model, X, y, feature_columns, device):\n        \"\"\"Analyze feature importance using correlation with last timestep\"\"\"\n        X_np = X.cpu().numpy()\n        y_np = y.cpu().numpy()\n        \n        n_samples = X_np.shape[0]\n        n_features = X_np.shape[2]\n        correlations = []\n        y_flat = y_np.flatten()\n\n        for i in range(n_features):\n            feature_data = X_np[:, -1, i]\n            \n            if np.std(feature_data) < 1e-10 or np.std(y_flat) < 1e-10:\n                corr = 0.0\n            else:\n                corr = np.corrcoef(feature_data, y_flat)[0, 1]\n                if np.isnan(corr):\n                    corr = 0.0\n            correlations.append(abs(corr))\n\n        feature_importance = pd.DataFrame({\n            'Feature': feature_columns,\n            'Importance': correlations\n        }).sort_values('Importance', ascending=False)\n\n        print(\"\\nFeature Importance Analysis Results:\")\n        print(feature_importance)\n        return feature_importance\n    \n    feature_importance = analyze_feature_importance(\n        base_model, \n        X_test_tensor, \n        y_test_tensor, \n        feature_columns, \n        device\n    )\n    \n    # Plot feature importance\n    plt.figure(figsize=(12, 8))\n    plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')\n    plt.title('Feature Importance Analysis')\n    plt.xlabel('Importance Score')\n    plt.ylabel('Feature')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/feature_importance_{timestamp}.png')\n    plt.close()\n    \n    # Plot feature correlation heatmap\n    plt.figure(figsize=(15, 12))\n    corr_matrix = data[feature_columns].corr()\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('Feature Correlation Heatmap')\n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/feature_correlation_heatmap_{timestamp}.png')\n    plt.close()\n    \n    # 2. Multi-model comparison experiment\n    print(\"\\n=== Multi-Model Comparison Experiment ===\")\n    models_to_compare = [\n        (\"Baseline GRU\", OptimizedGRU, {\n            'input_size': input_size,\n            'hidden_size': 32,\n            'num_layers': 1,\n            'dropout': 0.1\n        }),\n        (\"Optimized LSTM\", OptimizedLSTM, {\n            'input_size': input_size,\n            'hidden_size': 32,\n            'num_layers': 1,\n            'dropout': 0.1\n        }),\n        (\"Optimized Transformer\", OptimizedTransformer, {\n            'input_size': input_size,\n            'hidden_size': 32,\n            'num_layers': 1,\n            'num_heads': 2,\n            'dropout': 0.1\n        }),\n        (\"Optimized Attention LSTM\", OptimizedAttentionLSTM, {\n            'input_size': input_size,\n            'hidden_size': 32,\n            'num_layers': 1,\n            'dropout': 0.1\n        })\n    ]\n    \n    # Run multi-model comparison\n    model_results = run_model_comparison(\n        models_to_compare, \n        train_loader, test_loader, \n        X_train, y_train, X_test, y_test,\n        scaler, device, feature_columns\n    )\n    \n    # 3. Visualize comparison results\n    print(\"\\n=== Visualization Results ===\")\n    \n    # Multi-model metrics comparison\n    metrics_list = []\n    for model_name, result in model_results.items():\n        metrics = result['metrics']\n        metrics['Model'] = model_name\n        metrics_list.append(metrics)\n    \n    df = pd.DataFrame(metrics_list)\n    \n    metrics_to_compare = ['RMSE', 'MAE', 'MAPE', 'Direction Accuracy', 'R2']\n    \n    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n    axes = axes.flatten()\n    \n    for i, metric in enumerate(metrics_to_compare):\n        if i < len(axes) and metric in df.columns:\n            ax = axes[i]\n            df.plot(x='Model', y=metric, kind='bar', ax=ax, legend=False)\n            ax.set_title(f'{metric} Comparison')\n            ax.set_ylabel(metric)\n            ax.grid(True, alpha=0.3)\n            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n    \n    for i in range(len(metrics_to_compare), len(axes)):\n        fig.delaxes(axes[i])\n    \n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/model_metrics_comparison_{timestamp}.png')\n    plt.close()\n    \n    # Best model detailed analysis\n    best_model_name = min(model_results.keys(), key=lambda k: model_results[k]['metrics']['RMSE'])\n    print(f\"\\nBest Model: {best_model_name}\")\n    print(\"Best Model Metrics:\")\n    for metric, value in model_results[best_model_name]['metrics'].items():\n        if isinstance(value, (int, float)):\n            print(f\"{metric}: {value:.4f}\")\n        else:\n            print(f\"{metric}: {value}\")\n    \n    # Plot best model predictions\n    plt.figure(figsize=(18, 10))\n    plt.plot(test_dates, model_results[best_model_name]['actuals'], label='Actual Price', linewidth=3, color='blue', alpha=0.8)\n    plt.plot(test_dates, model_results[best_model_name]['predictions'], label='Predicted Price', linewidth=2, color='red', linestyle='--')\n    plt.title(f'AAPL Stock Price Prediction - {best_model_name}')\n    plt.xlabel('Date')\n    plt.ylabel('Close Price ($)')\n    plt.legend(fontsize=12)\n    plt.grid(True, alpha=0.3)\n    \n    metrics_text = f\"RMSE: {model_results[best_model_name]['metrics']['RMSE']:.2f}, MAE: {model_results[best_model_name]['metrics']['MAE']:.2f}, \"\n    metrics_text += f\"MAPE: {model_results[best_model_name]['metrics']['MAPE']:.2f}%, Direction Accuracy: {model_results[best_model_name]['metrics']['Direction Accuracy']:.2f}%\"\n    plt.figtext(0.5, 0.01, metrics_text, ha='center', fontsize=12, \n                bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n    \n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(f'/kaggle/working/best_model_prediction_{timestamp}.png')\n    plt.close()\n    \n    # 4. Compare predictions from all models\n    plt.figure(figsize=(20, 12))\n    plt.plot(test_dates, model_results[best_model_name]['actuals'], label='Actual Price', linewidth=4, color='black', alpha=0.9)\n    \n    colors = ['red', 'green', 'blue', 'purple']\n    linestyles = ['--', '-.', ':', '-']\n    \n    for i, (model_name, result) in enumerate(model_results.items()):\n        plt.plot(test_dates, result['predictions'], \n                label=model_name, \n                linewidth=2, \n                color=colors[i % len(colors)],\n                linestyle=linestyles[i % len(linestyles)],\n                alpha=0.7)\n    \n    plt.title('AAPL Stock Price Prediction - All Models Comparison')\n    plt.xlabel('Date')\n    plt.ylabel('Close Price ($)')\n    plt.legend(fontsize=12, loc='upper left')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/all_models_prediction_comparison_{timestamp}.png')\n    plt.close()\n    \n    print(\"\\n=== Experiment Completed ===\")\n    print(\"\\nOptimized AAPL stock price prediction model experiment completed!\")\n    print(f\"All plots have been saved to /kaggle/working/ directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T07:44:09.393146Z","iopub.execute_input":"2026-01-04T07:44:09.393483Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nLoading AAPL historical stock data...\n\nData shape: (2518, 6)\n\nFirst 5 rows:\n         Date Close/Last     Volume      Open      High       Low\n0  02/28/2020    $273.36  106721200   $257.26   $278.41   $256.37\n1  02/27/2020    $273.52   80151380    $281.1      $286   $272.96\n2  02/26/2020    $292.65   49678430   $286.53   $297.88    $286.5\n3  02/25/2020    $288.08   57668360   $300.95   $302.53   $286.13\n4  02/24/2020    $298.18   55548830   $297.26   $304.18   $289.23\n\nCalculating technical indicators...\n\nCleaned data shape: (2518, 18)\n\nBasic data info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2518 entries, 0 to 2517\nData columns (total 18 columns):\n #   Column         Non-Null Count  Dtype         \n---  ------         --------------  -----         \n 0   Date           2518 non-null   datetime64[ns]\n 1   Close          2518 non-null   float64       \n 2   Open           2518 non-null   float64       \n 3   High           2518 non-null   float64       \n 4   Low            2518 non-null   float64       \n 5   Volume         2518 non-null   int64         \n 6   MA5            2514 non-null   float64       \n 7   MA10           2509 non-null   float64       \n 8   MA20           2499 non-null   float64       \n 9   MA50           2469 non-null   float64       \n 10  RSI            2505 non-null   float64       \n 11  MACD           2518 non-null   float64       \n 12  Signal         2518 non-null   float64       \n 13  BB_Middle      2499 non-null   float64       \n 14  BB_Upper       2499 non-null   float64       \n 15  BB_Lower       2499 non-null   float64       \n 16  Volume_Change  2517 non-null   float64       \n 17  Price_Change   2517 non-null   float64       \ndtypes: datetime64[ns](1), float64(16), int64(1)\nmemory usage: 354.2 KB\nNone\n\nMissing values check:\nDate              0\nClose             0\nOpen              0\nHigh              0\nLow               0\nVolume            0\nMA5               4\nMA10              9\nMA20             19\nMA50             49\nRSI              13\nMACD              0\nSignal            0\nBB_Middle        19\nBB_Upper         19\nBB_Lower         19\nVolume_Change     1\nPrice_Change      1\ndtype: int64\n\nData normalization completed!\n\nFeature columns used (17):\n['Close', 'Open', 'High', 'Low', 'Volume', 'MA5', 'MA10', 'MA20', 'MA50', 'RSI', 'MACD', 'Signal', 'BB_Middle', 'BB_Upper', 'BB_Lower', 'Volume_Change', 'Price_Change']\nTotal data points: 2469\nTraining data size: 1975 (80.0%)\nTesting data size: 494 (20.0%)\n\nTraining set shapes - X: (1915, 60, 17), y: (1915,)\nTesting set shapes - X: (434, 60, 17), y: (434,)\n\nTensor shape verification:\nX_train_tensor: torch.Size([1915, 60, 17])\ny_train_tensor: torch.Size([1915])\nX_test_tensor: torch.Size([434, 60, 17])\ny_test_tensor: torch.Size([434])\n\nData preparation completed!\n\n=== Feature Importance Analysis ===\nStarting training BaseModel...\nEpoch 1/30, Train Loss: 0.002830, Test Loss: 0.004997, LR: 0.001000\nEpoch 2/30, Train Loss: 0.000168, Test Loss: 0.002311, LR: 0.001000\nEpoch 3/30, Train Loss: 0.000114, Test Loss: 0.001485, LR: 0.001000\nEpoch 4/30, Train Loss: 0.000089, Test Loss: 0.001382, LR: 0.001000\nEpoch 5/30, Train Loss: 0.000075, Test Loss: 0.000912, LR: 0.001000\nEpoch 6/30, Train Loss: 0.000065, Test Loss: 0.001054, LR: 0.001000\nEpoch 7/30, Train Loss: 0.000059, Test Loss: 0.000676, LR: 0.001000\nEpoch 8/30, Train Loss: 0.000055, Test Loss: 0.000603, LR: 0.001000\nEpoch 9/30, Train Loss: 0.000052, Test Loss: 0.000752, LR: 0.001000\nEpoch 10/30, Train Loss: 0.000050, Test Loss: 0.000494, LR: 0.001000\nEpoch 11/30, Train Loss: 0.000044, Test Loss: 0.000444, LR: 0.001000\nEpoch 12/30, Train Loss: 0.000045, Test Loss: 0.000393, LR: 0.001000\nEpoch 13/30, Train Loss: 0.000046, Test Loss: 0.000496, LR: 0.001000\nEpoch 14/30, Train Loss: 0.000044, Test Loss: 0.000473, LR: 0.001000\nEpoch 15/30, Train Loss: 0.000044, Test Loss: 0.000431, LR: 0.001000\n","output_type":"stream"}],"execution_count":null}]}